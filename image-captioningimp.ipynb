{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]\n",
      "keras version 2.10.0\n",
      "tensorflow version 2.10.1\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12680\\4234366435.py:16: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys, time, os, warnings \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"python {}\".format(sys.version))\n",
    "print(\"keras version {}\".format(keras.__version__)); del keras\n",
    "print(\"tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "# Configuring gpu for notebook\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "\n",
    "def set_seed(sd=123):\n",
    "    from numpy.random import seed\n",
    "    from tensorflow import set_random_seed\n",
    "    import random as rn\n",
    "    ## numpy random seed\n",
    "    seed(sd)\n",
    "    ## core python's random number \n",
    "    rn.seed(sd)\n",
    "    ## tensor flow's random number\n",
    "    set_random_seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jpg flies in Flicker8k: 8342\n"
     ]
    }
   ],
   "source": [
    "## The location of the Flickr8K_ photos\n",
    "dir_Flickr_jpg = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\Dataset\\\\presdataset\\\\Images\"\n",
    "## The location of the caption file\n",
    "dir_Flickr_text = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\Dataset\\\\presdataset\\\\edited captions.txt\"\n",
    "\n",
    "jpgs = os.listdir(dir_Flickr_jpg)\n",
    "print(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis\n",
    "\n",
    "#### Importing Caption data\n",
    "\n",
    "Load the text data and save it into a panda dataframe df_txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique file names : 1255\n",
      "The distribution of the number of captions for each image:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 1255})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read in the Flickr caption data\n",
    "file = open(dir_Flickr_text,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "datatxt = []\n",
    "for line in text.split('\\n'):\n",
    "    col = line.split('\\t')\n",
    "    if len(col) == 1:\n",
    "        continue\n",
    "    w = col[0].split(\"#\")\n",
    "    datatxt.append(w + [col[1].lower()])\n",
    "\n",
    "df_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"caption\"])\n",
    "\n",
    "\n",
    "uni_filenames = np.unique(df_txt.filename.values)\n",
    "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
    "print(\"The distribution of the number of captions for each image:\")\n",
    "\n",
    "# Counting number of captions for each image using counter\n",
    "Counter(Counter(df_txt.filename.values).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg,Two young guys with shaggy hair...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg,Two young White males are outsi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg,Two men in green shirts are sta...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg,A man in a blue shirt standing ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg,Two friends enjoy time spent to...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10002456.jpg,Several men in hard hats are oper...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename caption\n",
       "0  1000092795.jpg,Two young guys with shaggy hair...        \n",
       "1  1000092795.jpg,Two young White males are outsi...        \n",
       "2  1000092795.jpg,Two men in green shirts are sta...        \n",
       "3  1000092795.jpg,A man in a blue shirt standing ...        \n",
       "4  1000092795.jpg,Two friends enjoy time spent to...        \n",
       "5  10002456.jpg,Several men in hard hats are oper...        "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txt[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example pictures with captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Dell\\\\Desktop\\\\Dataset\\\\presdataset\\\\Images/1000092795.jpg,A man in a blue shirt standing in a garden .'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m filename \u001b[38;5;241m=\u001b[39m dir_Flickr_jpg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m jpgfnm\n\u001b[0;32m     13\u001b[0m captions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_txt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[df_txt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mjpgfnm]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m---> 14\u001b[0m image_load \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(npic,\u001b[38;5;241m2\u001b[39m,count,xticks\u001b[38;5;241m=\u001b[39m[],yticks\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m     17\u001b[0m ax\u001b[38;5;241m.\u001b[39mimshow(image_load)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:113\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not import PIL.Image. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    112\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe use of `load_img` requires PIL.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    114\u001b[0m     img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m color_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;66;03m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[39;00m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;66;03m# convert it to an 8-bit grayscale image.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Dell\\\\Desktop\\\\Dataset\\\\presdataset\\\\Images/1000092795.jpg,A man in a blue shirt standing in a garden .'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras_preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Displaying (npic = 5) pics from the dataset\n",
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "\n",
    "count = 1\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "for jpgfnm in uni_filenames[:npic]:\n",
    "    filename = dir_Flickr_jpg + '/' + jpgfnm\n",
    "    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    \n",
    "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "    \n",
    "    ax = fig.add_subplot(npic,2,count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,len(captions))\n",
    "    for i, caption in enumerate(captions):\n",
    "        ax.text(0,i,caption,fontsize=20)\n",
    "    count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_word(df_txt):\n",
    "    vocabulary = []\n",
    "    for txt in df_txt.caption.values:\n",
    "        vocabulary.extend(txt.split())\n",
    "    print('Vocabulary Size: %d' % len(set(vocabulary)))\n",
    "    ct = Counter(vocabulary)\n",
    "    dfword = pd.DataFrame(list(ct.items()), columns=['word', 'count'])\n",
    "    dfword.sort_values(by='count', ascending=False, inplace=True)\n",
    "    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n",
    "    return(dfword)\n",
    "dfword = df_word(df_txt)\n",
    "dfword.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The most and least frequently appearing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 50\n",
    "\n",
    "def plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.bar(dfsub.index,dfsub[\"count\"])\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "plthist(dfword.iloc[:topn,:],\n",
    "        title=\"The top 50 most frequently appearing words\")\n",
    "plthist(dfword.iloc[-topn:,:],\n",
    "        title=\"The least 50 most frequently appearing words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "text_original = \"I scored 21 in NLP, but Akshat topped. I have python v3.8. It's 4:20 pm. Could you give us 20 on 20?\"\n",
    "\n",
    "print(text_original)\n",
    "print(\"\\nRemove punctuations..\")\n",
    "def remove_punctuation(text_original):\n",
    "    text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n",
    "    return(text_no_punctuation)\n",
    "text_no_punctuation = remove_punctuation(text_original)\n",
    "print(text_no_punctuation)\n",
    "\n",
    "\n",
    "print(\"\\nRemove a single character word..\")\n",
    "def remove_single_character(text):\n",
    "    text_len_more_than1 = \"\"\n",
    "    for word in text.split():\n",
    "        if len(word) > 1:\n",
    "            text_len_more_than1 += \" \" + word\n",
    "    return(text_len_more_than1)\n",
    "text_len_more_than1 = remove_single_character(text_no_punctuation)\n",
    "print(text_len_more_than1)\n",
    "\n",
    "print(\"\\nRemove words with numeric values..\")\n",
    "def remove_numeric(text,printTF=False):\n",
    "    text_no_numeric = \"\"\n",
    "    for word in text.split():\n",
    "        isalpha = word.isalpha()\n",
    "        if printTF:\n",
    "            print(\"    {:10} : {:}\".format(word,isalpha))\n",
    "        if isalpha:\n",
    "            text_no_numeric += \" \" + word\n",
    "    return(text_no_numeric)\n",
    "text_no_numeric = remove_numeric(text_len_more_than1,printTF=True)\n",
    "print(text_no_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text_original):\n",
    "    text = remove_punctuation(text_original)\n",
    "    text = remove_single_character(text)\n",
    "    text = remove_numeric(text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "for i, caption in enumerate(df_txt.caption.values):\n",
    "    newcaption = text_clean(caption)\n",
    "    df_txt[\"caption\"].iloc[i] = newcaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfword = df_word(df_txt)\n",
    "plthist(dfword.iloc[:topn,:],\n",
    "        title=\"The top 50 most frequently appearing words\")\n",
    "plthist(dfword.iloc[-topn:,:],\n",
    "        title=\"The least 50 most frequently appearing words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "def add_start_end_seq_token(captions):\n",
    "    caps = []\n",
    "    for txt in captions:\n",
    "        txt = 'startseq ' + txt + ' endseq'\n",
    "        caps.append(txt)\n",
    "    return(caps)\n",
    "df_txt0 = copy(df_txt)\n",
    "df_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
    "df_txt0.head(5)\n",
    "del df_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preparation for the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvgg = tf.keras.applications.VGG16(include_top=True, weights=None)\n",
    "## load the locally saved weights \n",
    "modelvgg.load_weights(\"C:\\\\Users\\\\Dell\\\\Downloads\\\\vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n",
    "modelvgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvgg.layers.pop()\n",
    "modelvgg = tf.keras.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-2].output)\n",
    "## show the deep learning model\n",
    "modelvgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming images to features using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from collections import OrderedDict\n",
    "\n",
    "images = OrderedDict()\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "data = np.zeros((len(jpgs),npix,npix,3))\n",
    "for i,name in enumerate(jpgs):\n",
    "    # load an image from file\n",
    "    filename = dir_Flickr_jpg + '/' + name\n",
    "    image = load_img(filename, target_size=target_size)\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    nimage = preprocess_input(image)\n",
    "    \n",
    "    y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n",
    "    images[name] = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking the text and image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimages, keepindex = [],[]\n",
    "df_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\n",
    "for i, fnm in enumerate(df_txt0.filename):\n",
    "    if fnm in images.keys():\n",
    "        dimages.append(images[fnm])\n",
    "        keepindex.append(i)\n",
    "        \n",
    "fnames = df_txt0[\"filename\"].iloc[keepindex].values\n",
    "dcaptions = df_txt0[\"caption\"].iloc[keepindex].values\n",
    "dimages = np.array(dimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4096 features for all 8091 images\n",
    "\n",
    "dimages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of caption for each image\n",
    "\n",
    "dcaptions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Changing character vector to integer vector/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "## the maximum number of words in dictionary\n",
    "nb_words = 8000\n",
    "tokenizer = Tokenizer(nb_words=nb_words)\n",
    "tokenizer.fit_on_texts(dcaptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"vocabulary size : {}\".format(vocab_size))\n",
    "dtexts = tokenizer.texts_to_sequences(dcaptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtexts contains dcaptions but in token/integer form\n",
    "\n",
    "dtexts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset in training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the dataset in ratio 6:2:2 (train:valid:test)\n",
    "\n",
    "prop_test, prop_val = 0.2, 0.2\n",
    "\n",
    "N = len(dtexts)\n",
    "Ntest, Nval = int(N*prop_test), int(N*prop_val)\n",
    "\n",
    "def split_test_val_train(dtexts,Ntest,Nval):\n",
    "    return(dtexts[:Ntest], \n",
    "           dtexts[Ntest:Ntest+Nval],  \n",
    "           dtexts[Ntest+Nval:])\n",
    "\n",
    "dt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\n",
    "di_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\n",
    "fnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need maxlen because keras layer takes input of all the sequences of same length.\n",
    "# Hence to do the padding we need maximum length in caption\n",
    "\n",
    "maxlen = np.max([len(text) for text in dtexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def preprocessing(dtexts,dimages):\n",
    "    N = len(dtexts)\n",
    "    print(\"# captions/images = {}\".format(N))\n",
    "\n",
    "    assert(N==len(dimages))\n",
    "    Xtext, Ximage, ytext = [],[],[]\n",
    "    for text,image in zip(dtexts,dimages):\n",
    "\n",
    "        for i in range(1,len(text)):\n",
    "            in_text, out_text = text[:i], text[i]\n",
    "            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()\n",
    "            out_text = to_categorical(out_text,num_classes = vocab_size)\n",
    "\n",
    "            Xtext.append(in_text)\n",
    "            Ximage.append(image)\n",
    "            ytext.append(out_text)\n",
    "\n",
    "    Xtext  = np.array(Xtext)\n",
    "    Ximage = np.array(Ximage)\n",
    "    ytext  = np.array(ytext)\n",
    "    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n",
    "    return(Xtext,Ximage,ytext)\n",
    "\n",
    "\n",
    "Xtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\n",
    "Xtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)\n",
    "# pre-processing is not necessary for testing data\n",
    "#Xtext_test,  Ximage_test,  ytext_test  = preprocessing(dt_test,di_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First caption (dt_train[0]) represented in Xtext_train\n",
    "\n",
    "Xtext_train[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each word (49631 words excluding start token) there is a image associated with it\n",
    "# Image will be same for all words in a caption (i.e. if caption length is 14 Ximage_train[0] = Ximage_train[13])\n",
    "\n",
    "Ximage_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Words one-hot encoding (vocab-size = 4476)\n",
    "\n",
    "ytext_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ximage_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "#from tensorflow import keras\n",
    "#from keras import layers\n",
    "\n",
    "print(vocab_size)\n",
    "## image feature\n",
    "\n",
    "dim_embedding = 64\n",
    "\n",
    "input_image = layers.Input(shape=(Ximage_train.shape[1],))\n",
    "fimage = layers.Dense(256,activation='relu',name=\"ImageFeature\")(input_image)\n",
    "\n",
    "## sequence model\n",
    "input_txt = layers.Input(shape=(maxlen,))\n",
    "### The embedding layer in Keras can be used when we want to create the embeddings to embed higher dimensional data into lower dimensional vector space.\n",
    "ftxt = layers.Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\n",
    "ftxt = layers.LSTM(256,name=\"CaptionFeature\")(ftxt)\n",
    "\n",
    "## combined model for decoder\n",
    "decoder = layers.add([ftxt,fimage])\n",
    "decoder = layers.Dense(256,activation='relu')(decoder)\n",
    "output = layers.Dense(vocab_size,activation='softmax')(decoder)\n",
    "model = tf.keras.Model(inputs=[input_image, input_txt],outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We are giving our model an image and a word (Ximage_train & Xtext_train respectively) and we want it to predict the next word for it which is our target ytext_train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Here we are giving one image and a start-word and we want to predict next word.\n",
    "# The next word is in our target which is 75.\n",
    "\n",
    "print(Ximage_train[0])\n",
    "print(Xtext_train[0])\n",
    "print(ytext_train[0][75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "start = time.time()\n",
    "\n",
    "hist = model.fit([Ximage_train, Xtext_train], ytext_train, \n",
    "                  epochs=5, verbose=2, \n",
    "                  batch_size=64,\n",
    "                  validation_data=([Ximage_val, Xtext_val], ytext_val))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TIME TOOK {:3.2f}MIN\".format((end - start )/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ximage_train.shape,Xtext_train.shape,ytext_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\",\"val_loss\"]:\n",
    "    plt.plot(hist.history[label],label=label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
    "def predict_caption(image):\n",
    "    '''\n",
    "    image.shape = (1,4462)\n",
    "    '''\n",
    "\n",
    "    in_text = 'startseq'\n",
    "\n",
    "    for iword in range(maxlen):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence],maxlen)\n",
    "        yhat = model.predict([image,sequence],verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        newword = index_word[yhat]\n",
    "        in_text += \" \" + newword\n",
    "        if newword == \"endseq\":\n",
    "            break\n",
    "    return(in_text)\n",
    "\n",
    "\n",
    "\n",
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "\n",
    "count = 1\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "for jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n",
    "    ## images \n",
    "    filename = dir_Flickr_jpg + '/' + jpgfnm\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "\n",
    "    ## captions\n",
    "    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n",
    "    ax = fig.add_subplot(npic,2,count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.text(0,0.5,caption,fontsize=20)\n",
    "    count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['1000268201_693b08cb0e.jpg'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
    "\n",
    "\n",
    "nkeep = 5\n",
    "pred_good, pred_bad, bleus = [], [], [] \n",
    "count = 0 \n",
    "for jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n",
    "    count += 1\n",
    "    if count % 200 == 0:\n",
    "        print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n",
    "    \n",
    "    caption_true = [ index_word[i] for i in tokenized_text ]     \n",
    "    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n",
    "    ## captions\n",
    "    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n",
    "    caption = caption.split()\n",
    "    caption = caption[1:-1]## remove startreg, and endreg\n",
    "    \n",
    "    bleu = sentence_bleu([caption_true],caption)\n",
    "    bleus.append(bleu)\n",
    "    if bleu > 0.6 and len(pred_good) < nkeep:\n",
    "        pred_good.append((bleu,jpgfnm,caption_true,caption))\n",
    "    elif bleu < 0.3 and len(pred_bad) < nkeep:\n",
    "        pred_bad.append((bleu,jpgfnm,caption_true,caption))\n",
    "        \n",
    "end = time.time()\n",
    "print((start-end)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bad[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_images(pred_bad):\n",
    "    def create_str(caption_true):\n",
    "        strue = \"\"\n",
    "        for s in caption_true:\n",
    "            strue += \" \" + s\n",
    "        return(strue)\n",
    "    npix = 224\n",
    "    target_size = (npix,npix,3)    \n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10,20))\n",
    "    npic = len(pred_bad)\n",
    "    for pb in pred_bad:\n",
    "        bleu,jpgfnm,caption_true,caption = pb\n",
    "        ## images \n",
    "        filename = dir_Flickr_jpg + '/' + jpgfnm\n",
    "        image_load = load_img(filename, target_size=target_size)\n",
    "        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        caption_true = create_str(caption_true)\n",
    "        caption = create_str(caption)\n",
    "        \n",
    "        ax = fig.add_subplot(npic,2,count)\n",
    "        plt.axis('off')\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n",
    "        ax.text(0,0.4,\"pred:\" + caption,fontsize=20)\n",
    "        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n",
    "        count += 1\n",
    "    plt.show()\n",
    "\n",
    "print(\"Bad Caption\")\n",
    "plot_images(pred_bad)\n",
    "print(\"Good Caption\")\n",
    "plot_images(pred_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\Dell\\\\Desktop\\\\major project')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_img():\n",
    "    global img, image_data\n",
    "    for img_display in frame.winfo_children():\n",
    "        img_display.destroy()\n",
    "\n",
    "    image_data = filedialog.askopenfilename(initialdir=\"/\", title=\"Choose an image\",\n",
    "                                       filetypes=((\"all files\", \"*.*\"), (\"png files\", \"*.png\")))\n",
    "    basewidth = 300\n",
    "    img = Image.open(image_data)\n",
    "    wpercent = (basewidth / float(img.size[0]))\n",
    "    hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "    img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "    img = ImageTk.PhotoImage(img)\n",
    "    file_name = image_data.split('/')\n",
    "    panel = tk.Label(frame, text= str(file_name[len(file_name)-1]).upper()).pack()\n",
    "    panel_image = tk.Label(frame, image=img).pack()\n",
    "    \n",
    "def caption():\n",
    "    \n",
    "    original = Image.open(image_data)\n",
    "    original = original.resize((224, 224), Image.ANTIALIAS)\n",
    "    numpy_image = img_to_array(original)\n",
    "    nimage = preprocess_input(numpy_image)\n",
    "    \n",
    "    feature = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n",
    "    caption = predict_caption(feature)\n",
    "    table = tk.Label(frame, text=\"Caption: \" + caption[9:-7], font=(\"Helvetica\", 12)).pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root = tk.Tk()\n",
    "root=tk.Toplevel()\n",
    "root.title('IMAGE CAPTION GENERATOR')\n",
    "#root.iconbitmap('class.ico')\n",
    "ico = Image.open('test.jpeg')\n",
    "photo = ImageTk.PhotoImage(ico)\n",
    "root.wm_iconphoto(False, photo)\n",
    "root.resizable(False, False)\n",
    "tit = tk.Label(root, text=\"IMAGE CAPTION GENERATOR\", padx=25, pady=6, font=(\"\", 12)).pack()\n",
    "canvas = tk.Canvas(root, height=550, width=600, bg='#D1EDf2')\n",
    "canvas.pack()\n",
    "frame = tk.Frame(root, bg='white')\n",
    "frame.place(relwidth=0.8, relheight=0.8, relx=0.1, rely=0.1)\n",
    "chose_image = tk.Button(root, text='Choose Image',\n",
    "                        padx=35, pady=10,\n",
    "                        fg=\"black\", bg=\"pink\", command=upload_img, activebackground=\"#add8e6\")\n",
    "chose_image.pack(side=tk.LEFT)\n",
    "\n",
    "caption_image = tk.Button(root, text='Classify Image',\n",
    "                        padx=35, pady=10,\n",
    "                        fg=\"black\", bg=\"pink\", command=caption, activebackground=\"#add8e6\")\n",
    "caption_image.pack(side=tk.RIGHT)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
